{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13088429,"sourceType":"datasetVersion","datasetId":8290132}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"2d0a8a1c-5848-446f-ac14-2ad9c54dbf23","cell_type":"code","source":"# CHANGE THIS if your dataset folder name in /kaggle/input is different\nDATASET_DIR = \"/kaggle/input/indoor-fire-v1-rgb1-darknet\"\n\n# Working paths (don‚Äôt change)\nKAGGLE_WORKING = \"/kaggle/working\"\nEXTRACT_DIR    = f\"{KAGGLE_WORKING}/extracted\"\nBACKUP_DIR     = f\"{KAGGLE_WORKING}/backup\"\n\nimport os, shutil, glob, random, re, time, subprocess, sys\nos.makedirs(EXTRACT_DIR, exist_ok=True)\nos.makedirs(BACKUP_DIR, exist_ok=True)\nprint(\"DATASET_DIR:\", DATASET_DIR)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"58accd01-8d2e-4783-bf34-2752ed6dd665","cell_type":"code","source":"import os, sys, subprocess, textwrap\n\nprint(\"== nvidia-smi ==\")\nos.system(\"nvidia-smi || echo 'nvidia-smi not available'\")\n\nprint(\"\\n== CUDA toolchain ==\")\nos.system(\"nvcc --version || echo 'nvcc not available (OK if Darknet uses preinstalled CUDA)'\")\n\n# Make sure CUDA libs are visible (belt-and-suspenders)\nos.environ[\"LD_LIBRARY_PATH\"] = \"/usr/local/cuda/lib64:\" + os.environ.get(\"LD_LIBRARY_PATH\",\"\")\n\n# Hard fail if GPU isn‚Äôt present\nsmicode = os.system(\"nvidia-smi > /dev/null 2>&1\")\nif smicode != 0:\n    raise SystemExit(\"üö´ No GPU detected by nvidia-smi. Check Notebook -> Settings -> Accelerator -> GPU (T4).\")\nprint(\"‚úÖ GPU detected\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"08ca1269-96f3-41f4-a221-a102399925f0","cell_type":"code","source":"import os, shutil, subprocess, sys\n\n# make sure we are in a valid directory\nos.chdir(\"/kaggle/working\")\n\n# remove old clone if it exists\nshutil.rmtree(\"darknet\", ignore_errors=True)\n\nprint(\"== Cloning AlexeyAB/darknet ==\")\nres = subprocess.run(\n    [\"git\",\"clone\",\"--depth\",\"1\",\"https://github.com/AlexeyAB/darknet.git\",\"darknet\"],\n    stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True\n)\nprint(res.stdout[-400:])\nif res.returncode != 0:\n    print(res.stderr[-400:])\n    raise SystemExit(\"Git clone failed\")\n\nprint(\"‚úÖ Clone complete, darknet directory ready at /kaggle/working/darknet\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"12d091f3-218f-481a-a479-f7e2592a6f39","cell_type":"code","source":"import os, re, shutil, subprocess, textwrap, sys\n\nMK = \"/kaggle/working/darknet/Makefile\"\nassert os.path.exists(MK), \"Makefile not found; clone step must succeed first.\"\n\ntxt = open(MK,\"r\").read()\n\n# Ensure GPU/CUDNN/CUDNN_HALF are ON (leave OPENCV as-is for now)\ntxt = re.sub(r\"^GPU=0\",        \"GPU=1\",        txt, flags=re.M)\ntxt = re.sub(r\"^CUDNN=0\",      \"CUDNN=1\",      txt, flags=re.M)\ntxt = re.sub(r\"^CUDNN_HALF=0\", \"CUDNN_HALF=1\", txt, flags=re.M)\n\n# If you hit OpenCV pkg-config errors later, flip OPENCV to 0.\n# For now we keep it ON since your log shows it worked.\n# txt = re.sub(r\"^OPENCV=1\", \"OPENCV=0\", txt, flags=re.M)\n\n# Remove explicit -lcuda (not needed on Kaggle and causes link error)\n# Do it both in lines with LIBS and generic occurrences.\ntxt = re.sub(r\"\\s-lcuda(\\s|$)\", r\" \", txt)\n\n# Make sure arch includes sm_75 (T4). Keep existing others.\nif re.search(r\"^ARCH=\", txt, flags=re.M):\n    # append compute_75 if not present\n    if \"sm_75\" not in txt:\n        txt = re.sub(r\"^ARCH=.*\", \n                     lambda m: m.group(0) + \" -gencode arch=compute_75,code=[sm_75,compute_75]\", \n                     txt, flags=re.M)\nelse:\n    # Fallback: define ARCH line\n    txt += \"\\nARCH= -gencode arch=compute_75,code=[sm_75,compute_75]\\n\"\n\nopen(MK,\"w\").write(txt)\nprint(\"‚úÖ Makefile patched: GPU/CUDNN on, -lcuda removed, sm_75 ensured.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5ee1a141-c95a-4f09-a5fa-013f1d6ee60d","cell_type":"code","source":"import os, shutil, glob\n\n# Define your paths first\nDATASET_DIR  = \"/kaggle/input/indoor-fire-v1-rgb1-darknet\"  # <--- adjust if your dataset folder name changes\nEXTRACT_DIR  = \"/kaggle/working/extracted\"\nos.makedirs(EXTRACT_DIR, exist_ok=True)\n\nprint(\"== Copy dataset into working directory ==\")\nif not os.path.exists(DATASET_DIR):\n    os.system(\"ls -la /kaggle/input/\")\n    raise SystemExit(f\"‚ùå Dataset folder not found: {DATASET_DIR}\")\n\n# Shallow copy (keeps structure)\nfor item in os.listdir(DATASET_DIR):\n    s = os.path.join(DATASET_DIR, item)\n    d = os.path.join(EXTRACT_DIR, item)\n    if os.path.isdir(s):\n        if not os.path.exists(d):\n            shutil.copytree(s, d)\n    else:\n        shutil.copy2(s, d)\n\n# Find images that have corresponding YOLO txt labels\nimg_exts = (\".jpg\",\".jpeg\",\".png\",\".bmp\",\".JPG\",\".PNG\",\".JPEG\",\".BMP\")\nimgs = []\nfor ext in img_exts:\n    for p in glob.glob(os.path.join(EXTRACT_DIR,\"**\",f\"*{ext}\"), recursive=True):\n        if os.path.exists(os.path.splitext(p)[0] + \".txt\"):\n            imgs.append(os.path.abspath(p))\n\nprint(f\"‚úÖ Found labeled images: {len(imgs)}\")\nif len(imgs) < 10:\n    # show a quick peek to debug structure\n    os.system(f\"find {EXTRACT_DIR} -maxdepth 2 -type f | head -n 30\")\n    raise SystemExit(\"‚ùå Too few labeled images. Check that .txt label files sit beside images (same basename).\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"35af791b-22e2-44e3-ba21-dcf0f9c44dc9","cell_type":"code","source":"import os, random  # <-- add random here\n\n# Shuffle and split dataset\nrandom.shuffle(imgs)\nsplit = int(0.8 * len(imgs))\ntrain_list = imgs[:split]\nvalid_list = imgs[split:]\n\n# Create Darknet data folder\nos.makedirs(\"/kaggle/working/darknet/data\", exist_ok=True)\n\n# Write train/valid splits\nwith open(\"/kaggle/working/darknet/data/train.txt\",\"w\") as f:\n    f.write(\"\\n\".join(train_list))\nwith open(\"/kaggle/working/darknet/data/valid.txt\",\"w\") as f:\n    f.write(\"\\n\".join(valid_list))\n\n# Write class names\nnum_classes = 4\nwith open(\"/kaggle/working/darknet/data/obj.names\",\"w\") as f:\n    f.write(\"fire\\nno_fire\\nobjects\\nhuman\\n\")\n\n# Write data config\nBACKUP_DIR = \"/kaggle/working/backup\"  # define backup dir if not yet\nos.makedirs(BACKUP_DIR, exist_ok=True)\n\nwith open(\"/kaggle/working/darknet/data/obj.data\",\"w\") as f:\n    f.write(\n        f\"classes = {num_classes}\\n\"\n        f\"train = data/train.txt\\n\"\n        f\"valid = data/valid.txt\\n\"\n        f\"names = data/obj.names\\n\"\n        f\"backup = {BACKUP_DIR}\\n\"\n    )\n\nprint(f\"‚úÖ train: {len(train_list)}, valid: {len(valid_list)}\")\n\n# Quick path check\nwith open('/kaggle/working/darknet/data/train.txt') as f:\n    sample = [l.strip() for l in f.readlines()[:5] if l.strip()]\nprint(\"Sample paths:\", sample)\nfor sp in sample:\n    assert os.path.exists(sp), f\"Missing sample path: {sp}\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"53f15b15-0eb0-4f58-94d8-1b99fbbd0fae","cell_type":"code","source":"import shutil, glob, re, os\n\n# If num_classes isn't defined earlier, set it here:\ntry:\n    num_classes\nexcept NameError:\n    num_classes = 4  # adjust if needed\n\ncfg_dir = \"/kaggle/working/darknet/cfg\"\ncandidates = [\"yolov4-tiny-custom.cfg\",\"yolov4-tiny-3l.cfg\",\"yolov4-tiny.cfg\"]\nsrc_cfg = next((os.path.join(cfg_dir,c) for c in candidates if os.path.exists(os.path.join(cfg_dir,c))), None)\nif src_cfg is None:\n    ls = glob.glob(os.path.join(cfg_dir,\"*yolov4*tiny*.cfg\"))\n    if not ls:\n        os.system(f\"ls -la {cfg_dir}\")\n        raise SystemExit(\"‚ùå No yolov4-tiny cfg file found\")\n    src_cfg = ls[0]\n\ndst_cfg = os.path.join(cfg_dir,\"yolov4-tiny-obj.cfg\")\nshutil.copy2(src_cfg, dst_cfg)\n\n# ----- load cfg text -----\nwith open(dst_cfg, \"r\", errors=\"ignore\") as f:\n    text = f.read()\n\n# ----- robust key=value replacements (tolerant to spaces/casing) -----\ndef sub_kv(t, key, value):\n    # replace lines like \"key = something\" or \"key=something\"\n    pattern = re.compile(rf\"(?im)^\\s*{re.escape(key)}\\s*=\\s*.*$\")\n    if pattern.search(t):\n        return pattern.sub(f\"{key}={value}\", t)\n    else:\n        # if the key doesn't exist, insert near top (after first line)\n        lines = t.splitlines(True)\n        insert_at = min(10, len(lines))\n        lines.insert(insert_at, f\"{key}={value}\\n\")\n        return \"\".join(lines)\n\n# core training knobs\ntext = sub_kv(text, \"batch\", \"64\")\ntext = sub_kv(text, \"subdivisions\", \"32\")\ntext = sub_kv(text, \"width\", \"416\")\ntext = sub_kv(text, \"height\", \"416\")\n\n# schedule: force 8000 and 6400,7200\ntext = sub_kv(text, \"max_batches\", \"8000\")\ntext = sub_kv(text, \"steps\", \"6400,7200\")\n\n# optional learning rate if missing\nif re.search(r\"(?im)^\\s*learning_rate\\s*=\", text) is None:\n    lines = text.splitlines(True)\n    lines.insert(10, \"learning_rate=0.001\\n\")\n    text = \"\".join(lines)\n\n# ----- write back (so we can parse yolo blocks next) -----\nwith open(dst_cfg, \"w\") as f:\n    f.write(text)\n\n# ----- update classes and filters in [yolo] blocks -----\nwith open(dst_cfg, \"r\") as f:\n    lines = f.readlines()\n\nfilters = (num_classes + 5) * 3  # e.g., (4+5)*3 = 27\nyolo_idxs = [i for i,l in enumerate(lines) if l.strip().lower() == \"[yolo]\"]\n\nfor yi in yolo_idxs:\n    # set classes in the [yolo] block\n    j = yi + 1\n    while j < len(lines) and \"[\" not in lines[j]:\n        if lines[j].strip().lower().startswith(\"classes=\"):\n            lines[j] = f\"classes={num_classes}\\n\"; break\n        j += 1\n    # set filters in the preceding [convolutional] block\n    k = yi - 1\n    while k >= 0 and lines[k].strip().lower() != \"[convolutional]\":\n        k -= 1\n    if k >= 0:\n        m = k + 1\n        while m < yi:\n            if lines[m].strip().lower().startswith(\"filters=\"):\n                lines[m] = f\"filters={filters}\\n\"; break\n            m += 1\n\nwith open(dst_cfg, \"w\") as f:\n    f.writelines(lines)\n\nprint(\"‚úÖ cfg ready:\", dst_cfg)\n\n# quick verify (print the lines we care about)\nwith open(dst_cfg) as f:\n    cfg_txt = f.read()\nfor key in [\"batch\",\"subdivisions\",\"width\",\"height\",\"max_batches\",\"steps\"]:\n    m = re.search(rf\"(?im)^\\s*{key}\\s*=\\s*.*$\", cfg_txt)\n    if m: print(m.group(0))\nprint(\"classes/filters updated for all [yolo] heads.\")\n\n# download pretrain if needed\nif not os.path.exists(\"/kaggle/working/yolov4-tiny.conv.29\"):\n    rc = os.system(\n        \"wget -q https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v4_pre/yolov4-tiny.conv.29 \"\n        \"-O /kaggle/working/yolov4-tiny.conv.29\"\n    )\n    if rc != 0:\n        raise SystemExit(\"‚ùå Failed to download yolov4-tiny.conv.29\")\nprint(\"‚úÖ pretrain present\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"c379eba8-e1e4-4ee8-90c4-76ae5853b5a3","cell_type":"code","source":"import os, re\n\n# stay in your same path\nos.chdir(\"/kaggle/working/darknet\")\n\nprint(\"== Probe for driver libs ==\")\nos.system(\"ls -l /usr/local/cuda/compat/libcuda* 2>/dev/null || true\")\nos.system(\"ls -l /usr/local/cuda/lib64/stubs/libcuda* 2>/dev/null || true\")\n\nMK = \"Makefile\"\ntxt = open(MK, \"r\").read()\n\n# Ensure GPU/CUDNN/HALF ON; keep OpenCV OFF\ntxt = re.sub(r\"^GPU=0\\b\",        \"GPU=1\",        txt, flags=re.M)\ntxt = re.sub(r\"^CUDNN=0\\b\",      \"CUDNN=1\",      txt, flags=re.M)\ntxt = re.sub(r\"^CUDNN_HALF=0\\b\", \"CUDNN_HALF=1\", txt, flags=re.M)\ntxt = re.sub(r\"^OPENCV=1\\b\",     \"OPENCV=0\",     txt, flags=re.M)\n\n# Ensure sm_75 (T4) arch is included\nif re.search(r\"^ARCH\\s*=\", txt, flags=re.M):\n    if \"sm_75\" not in txt:\n        txt = re.sub(r\"^ARCH\\s*=.*\",\n                     lambda m: m.group(0) + \" -gencode arch=compute_75,code=[sm_75,compute_75]\",\n                     txt, flags=re.M)\nelse:\n    txt += \"\\nARCH= -gencode arch=compute_75,code=[sm_75,compute_75]\\n\"\n\n# Clean old link hacks so we can re-append cleanly\ntxt = re.sub(r\".*(kaggle driver link|kaggle link fix|driver link).*?\\n\", \"\", txt, flags=re.I)\ntxt = re.sub(r\".*(LIBS|LDFLAGS)\\s*\\+=.*-lcuda.*\\n\", \"\", txt)\ntxt = re.sub(r\".*(LIBS|LDFLAGS)\\s*\\+=.*(-lcudart|-lcublas|-lcurand|-lcudnn).*?\\n\", \"\", txt)\ntxt = re.sub(r\".*libcuda\\.so\\.1.*\\n\", \"\", txt)\ntxt = re.sub(r\".*/usr/lib/x86_64-linux-gnu.*\\n\", \"\", txt)\nopen(MK, \"w\").write(txt)\n\n# Decide which driver path to rpath\nhave_compat = os.path.exists(\"/usr/local/cuda/compat/libcuda.so\") or os.path.exists(\"/usr/local/cuda/compat/libcuda.so.1\")\nhave_stubs  = os.path.exists(\"/usr/local/cuda/lib64/stubs/libcuda.so\")\n\nrpath_line = \"\"\nif have_compat:\n    rpath_line = \"-Wl,-rpath,/usr/local/cuda/compat -L/usr/local/cuda/compat\"\nelif have_stubs:\n    rpath_line = \"-Wl,-rpath,/usr/local/cuda/lib64/stubs -L/usr/local/cuda/lib64/stubs\"\n\n# Force CUDA libs at the very end, grouped, and disable as-needed so nothing gets dropped\nfix = [\n    \"\\n# --- Kaggle driver/runtime link fix ---\\n\",\n    \"LDFLAGS+= -L/usr/local/cuda/lib64 -L/usr/local/cudnn/lib64 {rpath} \".format(rpath=rpath_line),\n    \"         -Wl,--no-as-needed -Wl,--start-group \",\n    \"         -lcudart -lcublas -lcurand -lcudnn -lcuda \",\n    \"         -Wl,--end-group\\n\",\n]\n\nwith open(MK, \"a\") as f:\n    f.write(\"\".join(fix))\n\nprint(\"‚úÖ Makefile patched (CUDA libs forced at end, grouped). Rebuilding...\")\n\n# Rebuild\nos.system(\"make clean > /dev/null 2>&1\")\nrc = os.system(\"make -j2 > build.log 2>&1\")\nprint(\"make rc=\", rc)\nif rc != 0:\n    print(\"‚ùå Build failed ‚Äî last 200 lines:\")\n    os.system(\"tail -n 200 build.log || cat build.log\")\n    raise SystemExit(\"Darknet GPU build failed. See log above.\")\n\n# Verify binary & linkage\nassert os.path.exists(\"./darknet\"), \"darknet binary missing after build\"\nos.system(\"chmod +x ./darknet\")\nprint(\"‚úÖ darknet binary ready\")\n\nprint(\"\\n== Verify libcuda & cudart mapping ==\")\nos.system(\"ldd ./darknet | egrep 'libcuda|libcudart|cublas|cudnn|curand' || true\")\n\nprint(\"\\n== Confirm link line had our group ==\")\nos.system(\"tail -n 200 build.log | egrep -i 'start-group|lcudart|lcublas|lcurand|lcuda|lcudnn|rpath' || true\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"2ba1d772-d977-4235-867c-811249033e81","cell_type":"code","source":"import os, sys, time, re, datetime, statistics, subprocess, shutil, tempfile, json\nfrom pathlib import Path\n\n# ===================== USER TOGGLE =====================\nEMERGENCY_ONLY = False   # <- set True to skip training and ONLY export the newest checkpoint\n# =======================================================\n\nos.chdir(\"/kaggle/working/darknet\")\n\n# --- paths (unchanged) ---\nCFG_PATH   = \"cfg/yolov4-tiny-obj.cfg\"\nDATA_PATH  = \"data/obj.data\"\nNAMES_PATH = \"data/obj.names\"\nBACKUP_DIR = \"/kaggle/working/backup\"     # Darknet default backup dir\nEXPORT_DIR = \"/kaggle/working\"             # where we drop last.weights + ZIP\n\n# --- read max_batches from cfg for ETA ---\nmax_batches = 8000\ntry:\n    with open(CFG_PATH) as f:\n        for ln in f:\n            m = re.search(r\"^\\s*max_batches\\s*=\\s*(\\d+)\", ln)\n            if m:\n                max_batches = int(m.group(1)); break\nexcept Exception:\n    pass\n\n# --- helper: find latest .weights in backup ---\ndef find_latest_weights(bk_dir=BACKUP_DIR):\n    if not os.path.isdir(bk_dir):\n        return None\n    ws = [str(p) for p in Path(bk_dir).glob(\"*.weights\")]\n    if not ws: return None\n    # prefer *_last.weights; else newest by mtime\n    last = [p for p in ws if p.endswith(\"_last.weights\")]\n    if last:\n        return max(last, key=os.path.getmtime)\n    return max(ws, key=os.path.getmtime)\n\n# --- helper: export newest checkpoint for download/resume ---\ndef export_latest_checkpoint(exp_dir=EXPORT_DIR, bk_dir=BACKUP_DIR, extra_note=None):\n    w = find_latest_weights(bk_dir)\n    if not w:\n        print(\"üö´ No .weights found to export in\", bk_dir)\n        return (None, None)\n\n    # Mirror to /kaggle/working/last.weights for easy resume in next runs\n    last_copy = os.path.join(exp_dir, \"last.weights\")\n    try:\n        shutil.copy2(w, last_copy)\n    except Exception as e:\n        print(\"‚ö†Ô∏è Could not copy to\", last_copy, \"->\", e)\n\n    # Prepare a small bundle with config + data for clean resume elsewhere\n    iter_tag = \"\"\n    m = re.search(r\"(\\d+)\\.weights$\", w)\n    if m: iter_tag = f\"_{m.group(1)}\"\n    bundle_name = f\"resume_yolov4tiny{iter_tag}\"  # no extension; make_archive adds .zip\n    bundle_root = tempfile.mkdtemp(prefix=\"resume_bundle_\")\n    try:\n        files_to_pack = []\n\n        # Required files if present\n        for pth in [w, CFG_PATH, DATA_PATH, NAMES_PATH, \"training.log\"]:\n            if os.path.exists(pth):\n                dst = os.path.join(bundle_root, os.path.basename(pth))\n                try:\n                    shutil.copy2(pth, dst)\n                    files_to_pack.append(dst)\n                except Exception as e:\n                    print(f\"‚ö†Ô∏è Skipped {pth}: {e}\")\n\n        # Add a tiny README with resume steps\n        readme = os.path.join(bundle_root, \"HOW_TO_RESUME.txt\")\n        with open(readme, \"w\") as f:\n            f.write(\nf\"\"\"HOW TO RESUME (Darknet):\n1) Put the .weights file into a 'backup' folder next to your cfg/data files.\n   Example structure:\n     /.../darknet/\n       ‚îú‚îÄ‚îÄ cfg/yolov4-tiny-obj.cfg\n       ‚îú‚îÄ‚îÄ data/obj.data\n       ‚îú‚îÄ‚îÄ data/obj.names\n       ‚îú‚îÄ‚îÄ backup/{os.path.basename(w)}\n2) Launch training with your usual command. Darknet will auto-pick the latest weights\n   if your script scans the backup folder; otherwise pass the weight path explicitly:\n   ./darknet detector train data/obj.data {CFG_PATH} backup/{os.path.basename(w)} -gpus 0\n\nNotes:\n- A copy of your latest weights is also placed at: {last_copy}\n- If your script already scans {BACKUP_DIR}, just keep the file there and rerun.\n\"\"\"\n            )\n        files_to_pack.append(readme)\n\n        # Optional metadata\n        meta = {\n            \"export_time\": datetime.datetime.now().isoformat(),\n            \"source_backup_dir\": bk_dir,\n            \"selected_weights\": w,\n            \"max_batches\": max_batches,\n            \"note\": extra_note or \"\"\n        }\n        meta_path = os.path.join(bundle_root, \"resume_meta.json\")\n        with open(meta_path, \"w\") as f:\n            json.dump(meta, f, indent=2)\n        files_to_pack.append(meta_path)\n\n        # Make ZIP in EXPORT_DIR\n        zip_base = os.path.join(exp_dir, bundle_name)\n        zip_path = shutil.make_archive(zip_base, \"zip\", bundle_root)\n        print(f\"‚úÖ Exported latest checkpoint:\\n   Weights copy : {last_copy}\\n   Bundle (ZIP) : {zip_path}\")\n        return (last_copy, zip_path)\n    finally:\n        try:\n            shutil.rmtree(bundle_root, ignore_errors=True)\n        except: pass\n\n# --- emergency-only mode: just export & exit ---\nif EMERGENCY_ONLY:\n    export_latest_checkpoint(extra_note=\"Manual emergency export (EMERGENCY_ONLY=True).\")\n    raise SystemExit(0)\n\n# --- sanity: built with GPU? ---\nout = subprocess.run([\"./darknet\",\"detector\",\"train\",\"-h\"],\n                     stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True).stdout\nprint(out.splitlines()[0] if out else \"\")\nif \"GPU isn't used\" in out:\n    raise SystemExit(\"‚ùå Darknet binary is CPU-only. Rebuild with GPU flags before training.\")\n\n# --- config: turn mAP on/off ---\nUSE_MAP = False   # <- set True when you want periodic mAP\n\n# --- choose weights (resume if available) ---\nweights_file = \"/kaggle/working/yolov4-tiny.conv.29\"\nif os.path.isdir(BACKUP_DIR):\n    import glob\n    w = sorted(glob.glob(os.path.join(BACKUP_DIR,\"*.weights\")), key=os.path.getmtime)\n    if w:\n        weights_file = w[-1]  # resume from latest checkpoint\n\n# --- training command (streamed) ---\ncmd = [\"./darknet\",\"detector\",\"train\", DATA_PATH, CFG_PATH, weights_file, \"-gpus\",\"0,1\"]\nif USE_MAP:\n    cmd.insert(6, \"-map\")  # add mAP right before -gpus\n\nprint(\"== Training cmd ==\\n\", \" \".join(cmd))\n\n# --- regex to parse progress lines & weight saves ---\niter_re = re.compile(r\"^\\s*(\\d+):\\s*([\\d.]+),\\s*([\\d.]+)\\s*avg loss.*?([0-9.]+)\\s*seconds\", re.I)\nsave_re = re.compile(r\"Saving weights to\\s+(.+?\\.weights)\\s*$\")\n\nstart = time.time()\nlast_status_print = 0.0\nsecs_hist = []\nlast_map_line = None\nlast_saved_weights = None\n\nwith open(\"training.log\", \"a\", buffering=1) as logf:\n    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n                            text=True, bufsize=1, universal_newlines=True)\n    try:\n        for line in proc.stdout:\n            logf.write(line)\n            s = line.rstrip()\n\n            # progress\n            m = iter_re.search(s)\n            if m:\n                cur_iter = int(m.group(1))\n                avg_loss = float(m.group(3))\n                sec_it   = float(m.group(4))\n                secs_hist.append(sec_it)\n                if len(secs_hist) > 50: secs_hist = secs_hist[-50:]\n                sec_per_iter = statistics.mean(secs_hist)\n                eta_secs = max(0, int((max_batches - cur_iter) * sec_per_iter))\n                pct = 100.0 * cur_iter / max_batches\n                bar_len, filled = 28, int(28 * pct / 100)\n                bar = \"‚ñà\"*filled + \"-\"*(bar_len-filled)\n                now = time.time()\n                if now - last_status_print > 3:\n                    print(f\"[{cur_iter:>6}/{max_batches:<6}] [{bar}] {pct:5.1f}% | avg loss {avg_loss:.4f} | {sec_per_iter:.2f}s/it | ETA {datetime.timedelta(seconds=eta_secs)}\")\n                    last_status_print = now\n\n            # mAP lines\n            if USE_MAP and (\"mAP@\" in s):\n                last_map_line = s\n                print(\"üìà\", s)\n\n            # weight saving events -> immediately mirror + mini-export\n            msave = save_re.search(s)\n            if msave:\n                saved_path = msave.group(1).strip()\n                last_saved_weights = saved_path\n                print(\"üíæ\", s)\n\n                # Mirror the just-saved weights to /kaggle/working/last.weights\n                try:\n                    shutil.copy2(saved_path, os.path.join(EXPORT_DIR, \"last.weights\"))\n                except Exception as e:\n                    print(\"‚ö†Ô∏è Could not mirror to last.weights:\", e)\n\n                # Lightweight export: update a tiny JSON pointer (handy after crashes)\n                try:\n                    with open(os.path.join(EXPORT_DIR, \"last_checkpoint.json\"), \"w\") as fp:\n                        json.dump({\"last_saved_weights\": saved_path,\n                                   \"time\": datetime.datetime.now().isoformat()}, fp)\n                except Exception as e:\n                    print(\"‚ö†Ô∏è Could not write last_checkpoint.json:\", e)\n\n            if (\"nan\" in s.lower()) or (\"CUDA Error\" in s) or (\"No such file\" in s):\n                print(\"‚ö†Ô∏è\", s)\n\n        proc.wait()\n    finally:\n        try: proc.terminate()\n        except: pass\n\nrc = proc.returncode\nprint(f\"\\n== Training finished. Exit code: {rc}, elapsed: {(time.time()-start)/60:.1f} min ==\")\n\n# --- always do a final export so you can download even if training stopped ---\n_ = export_latest_checkpoint(extra_note=f\"Final export after training exit (rc={rc}).\")\n\nif rc != 0:\n    print(\"‚ùå Training failed ‚Äî tail of training.log:\")\n    os.system(\"tail -n 150 training.log || true\")\nelse:\n    print(\"\\n== Recent 'Saving weights to' ==\")\n    os.system(\"grep -n 'Saving weights to' training.log | tail -n 5 || true\")\n    if USE_MAP and last_map_line:\n        print(\"Last mAP line:\", last_map_line)\n\nprint(\"\\nüëâ To do a manual emergency export without training, set EMERGENCY_ONLY=True and rerun this cell.\")\nprint(\"üì¶ Download paths you‚Äôll see in the file browser:\\n  - /kaggle/working/last.weights\\n  - /kaggle/working/resume_yolov4tiny_*.zip\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f205a01b-0391-4ed3-b49e-c28a8b211b2e","cell_type":"code","source":"#BELOW ARE FOR TESTING ACCURACY AND PRECISION","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d4a335b1-9ab2-49bb-a7e9-5615ffa23e5a","cell_type":"code","source":"#mAP on your validation set (per-class AP + overall mAP)\ncd /kaggle/working/darknet\n\n# pick the weights you want to evaluate\nWEIGHTS=\"/kaggle/working/last.weights\"      # or backup/yolov4-tiny-obj_best.weights if you have it\n\n# mAP @ IoU=0.50 (PASCAL VOC style) with dense PR curve (101 points)\n./darknet detector map data/obj.data cfg/yolov4-tiny-obj.cfg \"$WEIGHTS\" \\\n  -iou_thresh 0.50 -thresh 0.25 -points 101 -dont_show > map.log 2>&1\n\n# show the key lines\ntail -n 60 map.log\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"fd816682-33ba-4fab-b616-f4b46c3ae3f5","cell_type":"code","source":"#Quick recall/precision snapshot (fast sanity check)\ncd /kaggle/working/darknet\n./darknet detector recall data/obj.data cfg/yolov4-tiny-obj.cfg \"$WEIGHTS\" \\\n  -thresh 0.25 -iou_thresh 0.50 > recall.log 2>&1\ntail -n 40 recall.log","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}